{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Classification</h1>\n",
    "\n",
    "Classifying articles based on <i>'usefulness'</i> to the domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import naive_bayes, linear_model, svm\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Article 1</th>\n",
       "      <td>Pier 1 appoints interim CFO amid growing finan...</td>\n",
       "      <td>Pier on Wednesday reported that fourth quarter...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 2</th>\n",
       "      <td>Family Dollar to close nearly 400 stores</td>\n",
       "      <td>Dollar Tree on Wednesday announced that up to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 3</th>\n",
       "      <td>Having to share personal data turns consumers ...</td>\n",
       "      <td>According to a new Harris Poll survey of Ameri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 4</th>\n",
       "      <td>Walgreens taps Narvar for online pickup return...</td>\n",
       "      <td>Customer experience platform Narvar and Walgre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 5</th>\n",
       "      <td>TechStyle claims more than 5M active members</td>\n",
       "      <td>TechStyle Fashion Group which operates ShoeDaz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       title  \\\n",
       "number                                                         \n",
       "Article 1  Pier 1 appoints interim CFO amid growing finan...   \n",
       "Article 2           Family Dollar to close nearly 400 stores   \n",
       "Article 3  Having to share personal data turns consumers ...   \n",
       "Article 4  Walgreens taps Narvar for online pickup return...   \n",
       "Article 5       TechStyle claims more than 5M active members   \n",
       "\n",
       "                                                     content  label  \n",
       "number                                                               \n",
       "Article 1  Pier on Wednesday reported that fourth quarter...      1  \n",
       "Article 2  Dollar Tree on Wednesday announced that up to ...      0  \n",
       "Article 3  According to a new Harris Poll survey of Ameri...      1  \n",
       "Article 4  Customer experience platform Narvar and Walgre...      0  \n",
       "Article 5  TechStyle Fashion Group which operates ShoeDaz...      1  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the dataset\n",
    "#Target variable is labelled with 1 as 'Useful' and 0 as 'Not Useful'\n",
    "\n",
    "data = pd.read_excel(\"Data - Text Classification.xlsx\", usecols=[0,1,3,5])\n",
    "data.set_index('number', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp.Defaults.stop_words |= {\"company\", \"companies\", \"companys\"}\n",
    "data['tokenized_content'] = data['content'].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "for doc in nlp.pipe(data['tokenized_content'].astype('unicode').values, batch_size=50, n_threads=3):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.lemma_ for n in doc if n.is_stop == False])      \n",
    "    else:\n",
    "        tokens.append(None)\n",
    "\n",
    "data['tokenized_content'] = tokens\n",
    "data['tokenized_content'] = data['tokenized_content'].apply(','.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 3)\n",
      "(47,)\n",
      "(9, 3)\n",
      "(9,)\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(\"label\", axis = 1), data['label'], \n",
    "                                                    test_size = 0.15, random_state = 99)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "cv = CountVectorizer(lowercase=False, stop_words='english')\n",
    "cv.fit(X_train['tokenized_content'])\n",
    "\n",
    "X_train_count = cv.transform(X_train['tokenized_content'])\n",
    "X_test_count = cv.transform(X_test['tokenized_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using TFIDF vectorizer\n",
    "tfidf = TfidfVectorizer(lowercase=False, ngram_range=(1,2))\n",
    "tfidf.fit(X_train['tokenized_content'])\n",
    "                        \n",
    "X_train_tfidf = tfidf.transform(X_train['tokenized_content'])\n",
    "X_test_tfidf = tfidf.transform(X_test['tokenized_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(X_train['tokenized_content'])\n",
    "\n",
    "train_sequences = pad_sequences(tokenizer.texts_to_sequences(X_train['tokenized_content']), maxlen=50)\n",
    "test_sequences = pad_sequences(tokenizer.texts_to_sequences(X_test['tokenized_content']), maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('GloVe/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(classifier, train_features, train_label, test_features):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(train_features, train_label)\n",
    "    \n",
    "    # predict the labels on test dataset\n",
    "    predictions = classifier.predict(test_features)\n",
    "\n",
    "    return predictions, accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes using CountVectorizer - Prediction: [0 1 0 0 0 1 0 1 1]\n",
      "Naive Bayes using CountVectorizer- Accuracy: 0.8889\n",
      "Naive Bayes using TF-IDF Vectorizer - Prediction: [1 1 1 1 1 1 1 1 1]\n",
      "Naive Bayes using TF-IDF Vectorizer- Accuracy: 0.5556\n"
     ]
    }
   ],
   "source": [
    "prediction, accuracy = model(naive_bayes.MultinomialNB(), X_train_count, y_train, X_test_count)\n",
    "print(\"Naive Bayes using CountVectorizer - Prediction:\", prediction)\n",
    "print(\"Naive Bayes using CountVectorizer- Accuracy:\", round(accuracy,4))\n",
    "\n",
    "prediction, accuracy = model(naive_bayes.MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print(\"Naive Bayes using TF-IDF Vectorizer - Prediction:\", prediction)\n",
    "print(\"Naive Bayes using TF-IDF Vectorizer- Accuracy:\", round(accuracy,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number\n",
       "Article 50    0\n",
       "Article 17    1\n",
       "Article 8     0\n",
       "Article 42    0\n",
       "Article 7     1\n",
       "Article 44    1\n",
       "Article 31    0\n",
       "Article 15    1\n",
       "Article 52    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression using CountVectorizer - Prediction: [0 0 0 0 0 1 1 1 1]\n",
      "Logistic Regression using CountVectorizer - Accuracy: 0.6667\n",
      "Logistic Regression using TF-IDF Vectorizer - Prediction: [1 1 1 0 0 1 0 1 1]\n",
      "Logistic Regression using TF-IDF Vectorizer- Accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "prediction, accuracy = model(linear_model.LogisticRegression(), X_train_count, y_train, X_test_count)\n",
    "print(\"Logistic Regression using CountVectorizer - Prediction:\", prediction)\n",
    "print(\"Logistic Regression using CountVectorizer - Accuracy:\", round(accuracy,4))\n",
    "\n",
    "prediction, accuracy = model(linear_model.LogisticRegression(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print(\"Logistic Regression using TF-IDF Vectorizer - Prediction:\", prediction)\n",
    "print(\"Logistic Regression using TF-IDF Vectorizer- Accuracy:\", round(accuracy,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Support Vector Classifier(SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC using CountVectorizer - Prediction: [1 1 1 1 1 1 1 1 1]\n",
      "SVC using CountVectorizer - Accuracy: 0.5556\n",
      "SVC using TF-IDF Vectorizer - Prediction: [1 1 1 1 1 1 1 1 1]\n",
      "SVC using TF-IDF Vectorizer- Accuracy: 0.5556\n"
     ]
    }
   ],
   "source": [
    "prediction, accuracy = model(svm.SVC(), X_train_count, y_train, X_test_count)\n",
    "print(\"SVC using CountVectorizer - Prediction:\", prediction)\n",
    "print(\"SVC using CountVectorizer - Accuracy:\", round(accuracy,4))\n",
    "\n",
    "prediction, accuracy = model(svm.SVC(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print(\"SVC using TF-IDF Vectorizer - Prediction:\", prediction)\n",
    "print(\"SVC using TF-IDF Vectorizer- Accuracy:\", round(accuracy,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Recurrent Neural Network - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_lstm.compile(loss = \"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47 samples, validate on 9 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.6902 - acc: 0.562 - 2s 46ms/step - loss: 0.6973 - acc: 0.5106 - val_loss: 0.6844 - val_acc: 0.6667\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.6870 - acc: 0.468 - 0s 997us/step - loss: 0.6777 - acc: 0.5106 - val_loss: 0.6678 - val_acc: 0.5556\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.6362 - acc: 0.656 - 0s 1ms/step - loss: 0.6403 - acc: 0.6170 - val_loss: 0.6529 - val_acc: 0.5556\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.6213 - acc: 0.625 - 0s 997us/step - loss: 0.6291 - acc: 0.6170 - val_loss: 0.6384 - val_acc: 0.5556\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.6274 - acc: 0.687 - 0s 997us/step - loss: 0.6243 - acc: 0.6809 - val_loss: 0.6225 - val_acc: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x283ecd94550>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(train_sequences, y_train, validation_data=(test_sequences, y_test), epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LSTM with Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\n",
    "model_conv.add(Dropout(0.2))\n",
    "model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "model_conv.add(MaxPooling1D(pool_size=4))\n",
    "model_conv.add(LSTM(100))\n",
    "model_conv.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47 samples, validate on 9 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.7159 - acc: 0.406 - 2s 47ms/step - loss: 0.7204 - acc: 0.4043 - val_loss: 0.6693 - val_acc: 0.5556\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.6682 - acc: 0.531 - 0s 664us/step - loss: 0.6614 - acc: 0.5532 - val_loss: 0.6753 - val_acc: 0.5556\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.6345 - acc: 0.843 - 0s 333us/step - loss: 0.6353 - acc: 0.8723 - val_loss: 0.6755 - val_acc: 0.5556\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.6040 - acc: 0.750 - 0s 664us/step - loss: 0.6240 - acc: 0.6809 - val_loss: 0.6642 - val_acc: 0.5556\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.5908 - acc: 0.750 - 0s 665us/step - loss: 0.5780 - acc: 0.7447 - val_loss: 0.6551 - val_acc: 0.4444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x283ecdc4b00>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.fit(train_sequences, y_train, validation_data=(test_sequences, y_test), epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Naive Bayes with CountVectorizer has better accuracy compared to other models</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
